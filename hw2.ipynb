{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": 1,
    "id": "kr9vAeEQlRVG"
   },
   "source": [
    "# Домашнее задание 2. Классификация изображений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": 3,
    "id": "BxX49gLclRVJ"
   },
   "source": [
    "В этом задании потребуется обучить классификатор изображений. Будем работать с датасетом, название которого раскрывать не будем. Можете посмотреть самостоятельно на картинки, которые в есть датасете. В нём 200 классов и около 5 тысяч картинок на каждый класс. Классы пронумерованы, как нетрудно догадаться, от 0 до 199. Скачать датасет можно вот [тут](https://yadi.sk/d/BNR41Vu3y0c7qA).\n",
    "\n",
    "Структура датасета простая -- есть директории train/ и val/, в которых лежат обучающие и валидационные данные. В train/ и val/ лежат директориии, соответствующие классам изображений, в которых лежат, собственно, сами изображения.\n",
    " \n",
    "__Задание__. Необходимо выполнить любое из двух заданий\n",
    "\n",
    "1) Добейтесь accuracy **на валидации не менее 0.44**. В этом задании **запрещено** пользоваться предобученными моделями и ресайзом картинок. \n",
    "\n",
    "2) Добейтесь accuracy **на валидации не менее 0.84**. В этом задании делать ресайз и использовать претрейн можно. \n",
    "\n",
    "Напишите краткий отчёт о проделанных экспериментах. Что сработало и что не сработало? Почему вы решили, сделать так, а не иначе? Обязательно указывайте ссылки на чужой код, если вы его используете. Обязательно ссылайтесь на статьи / блогпосты / вопросы на stackoverflow / видосы от ютуберов-машинлернеров / курсы / подсказки от Дяди Васи и прочие дополнительные материалы, если вы их используете. \n",
    "\n",
    "Ваш код обязательно должен проходить все `assert`'ы ниже.\n",
    "\n",
    "Необходимо написать функции `train_one_epoch`, `train` и `predict` по шаблонам ниже (во многом повторяют примеры с семинаров).Обратите особое внимание на функцию `predict`: она должна возвращать список лоссов по всем объектам даталоадера, список предсказанных классов для каждого объекта из даталоалера и список настоящих классов для каждого объекта в даталоадере (и именно в таком порядке).\n",
    "\n",
    "__Использовать внешние данные для обучения строго запрещено в обоих заданиях. Также запрещено обучаться на валидационной выборке__.\n",
    "\n",
    "\n",
    "__Критерии оценки__: Оценка вычисляется по простой формуле: `min(10, 10 * Ваша accuracy / 0.44)` для первого задания и `min(10, 10 * (Ваша accuracy - 0.5) / 0.34)` для второго. Оценка округляется до десятых по арифметическим правилам. Если вы выполнили оба задания, то берется максимум из двух оценок.\n",
    "\n",
    "__Бонус__. Вы получаете 5 бонусных баллов если справляетесь с обоими заданиями на 10 баллов (итого 15 баллов). В противном случае выставляется максимальная из двух оценок и ваш бонус равен нулю.\n",
    "\n",
    "__Советы и указания__:\n",
    " - Наверняка вам потребуется много гуглить о классификации и о том, как заставить её работать. Это нормально, все гуглят. Но не забывайте, что нужно быть готовым за скатанный код отвечать :)\n",
    " - Используйте аугментации. Для этого пользуйтесь модулем `torchvision.transforms` или библиотекой [albumentations](https://github.com/albumentations-team/albumentations)\n",
    " - Можно обучать с нуля или файнтюнить (в зависимости от задания) модели из `torchvision`.\n",
    " - Рекомендуем написать вам сначала класс-датасет (или воспользоваться классом `ImageFolder`), который возвращает картинки и соответствующие им классы, а затем функции для трейна по шаблонам ниже. Однако делать это мы не заставляем. Если вам так неудобно, то можете писать код в удобном стиле. Однако учтите, что чрезмерное изменение нижеперечисленных шаблонов увеличит количество вопросов к вашему коду и повысит вероятность вызова на защиту :)\n",
    " - Валидируйте. Трекайте ошибки как можно раньше, чтобы не тратить время впустую.\n",
    " - Чтобы быстро отладить код, пробуйте обучаться на маленькой части датасета (скажем, 5-10 картинок просто чтобы убедиться что код запускается). Когда вы поняли, что смогли всё отдебажить, переходите обучению по всему датасету\n",
    " - На каждый запуск делайте ровно одно изменение в модели/аугментации/оптимайзере, чтобы понять, что и как влияет на результат.\n",
    " - Фиксируйте random seed.\n",
    " - Начинайте с простых моделей и постепенно переходите к сложным. Обучение лёгких моделей экономит много времени.\n",
    " - Ставьте расписание на learning rate. Уменьшайте его, когда лосс на валидации перестаёт убывать.\n",
    " - Советуем использовать GPU. Если у вас его нет, используйте google colab. Если вам неудобно его использовать на постоянной основе, напишите и отладьте весь код локально на CPU, а затем запустите уже написанный ноутбук в колабе. Авторское решение задания достигает требуемой точности в колабе за 15 минут обучения.\n",
    " \n",
    "Good luck & have fun! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_id": 4,
    "id": "LKcSNj4tlRVK"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from os.path import isfile, join\n",
    "import PIL\n",
    "import os\n",
    "from PIL import Image\n",
    "from sklearn.metrics import accuracy_score\n",
    "from typing import Dict, List, Tuple, Union\n",
    "# You may add any imports you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-11-14 13:26:11--  https://disk.yandex.ru/d/BNR41Vu3y0c7qA\n",
      "Распознаётся disk.yandex.ru (disk.yandex.ru)… 87.250.250.50\n",
      "Подключение к disk.yandex.ru (disk.yandex.ru)|87.250.250.50|:443... соединение установлено.\n",
      "HTTP-запрос отправлен. Ожидание ответа… 200 OK\n",
      "Длина: 25044 (24K) [text/html]\n",
      "Сохранение в: «BNR41Vu3y0c7qA.7»\n",
      "\n",
      "BNR41Vu3y0c7qA.7    100%[===================>]  24,46K  --.-KB/s    за 0s      \n",
      "\n",
      "2021-11-14 13:26:12 (63,5 MB/s) - «BNR41Vu3y0c7qA.7» сохранён [25044/25044]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://disk.yandex.ru/d/BNR41Vu3y0c7qA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image format: JPEG; shape: (64, 64); color scheme: RGB\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=64x64 at 0x7FB00445F460>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = Image.open('dataset/train/class_095/00000.jpg')\n",
    "print(f\"Image format: {img.format}; shape: {img.size}; color scheme: {img.mode}\")\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"dataset/train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images_dir = []\n",
    "\n",
    "# for f in os.listdir(path):\n",
    "#     root_class = path + f'/{f}'\n",
    "#     if f == '.DS_Store':\n",
    "#         continue\n",
    "#     print(root_class)\n",
    "#     for im in os.listdir(root_class):\n",
    "#         if isfile(join(root_class, im)):\n",
    "#             images_dir.append(join(root_class, im))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RytEDW0ylRVN"
   },
   "source": [
    "### Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset_(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str,\n",
    "        transform: transforms.Compose = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        :param root: путь к папке с данными\n",
    "        :param transform: transforms of the images and labels\n",
    "        \"\"\"\n",
    "        self.root = os.path.expanduser(root)\n",
    "        self.transform = transform\n",
    "        (self.data_path, self.labels_path) = ([], [])\n",
    "\n",
    "        def load_images(path: str) -> List[str]:\n",
    "            \"\"\"\n",
    "            Возвращает список с путями до всех изображений\n",
    "\n",
    "            :param path: путь к папке с данными\n",
    "            :return: лист с путями до всех изображений\n",
    "            \"\"\"\n",
    "            images_dir = []\n",
    "            \n",
    "            for f in os.listdir(path):\n",
    "                if f == '.DS_Store':\n",
    "                    continue\n",
    "                root_class = path + f'/{f}'\n",
    "                for im in os.listdir(root_class):\n",
    "                    if isfile(join(root_class, im)):\n",
    "                        images_dir.append(join(root_class, im))\n",
    "            return images_dir\n",
    "\n",
    "        self.data_path = load_images(self.root)\n",
    "\n",
    "    def __getitem__(self, index: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        :param index: sample index\n",
    "        :return: tuple (img, target) with the input data and its label\n",
    "        \"\"\"\n",
    "        img = Image.open(self.data_path[index])\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return tuple([img, int(self.data_path[index][-13:-10])])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_path)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "cell_id": 5,
    "id": "QEdDQtHdlRVO"
   },
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import Compose, Normalize, Resize, ToTensor \n",
    "# 1-ый вариант\n",
    "\n",
    "dataset = ImageFolder(\n",
    "    \"./dataset/train\", \n",
    "    transform=Compose(\n",
    "        [ \n",
    "            ToTensor(), \n",
    "            Normalize((0.5, 0.5, 0.5), (1, 1, 1)), \n",
    "        ]\n",
    "    )\n",
    ")\n",
    "train_set, test_set = torch.utils.data.random_split(\n",
    "    dataset, \n",
    "    [int(0.8 * len(dataset)), len(dataset) - int(0.8 * len(dataset))]\n",
    ")\n",
    "\n",
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ColorJitter(hue=.05, saturation=.05),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(20, resample=PIL.Image.BILINEAR),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ]\n",
    ")\n",
    "val_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ColorJitter(hue=.05, saturation=.05),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(20, resample=PIL.Image.BILINEAR),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ]\n",
    ")\n",
    "# YOU CAN DEFINE AUGMENTATIONS HERE\n",
    "\n",
    "train_dataset = MyDataset_(\"dataset/train\", transform=train_transform)\n",
    "val_dataset = MyDataset_(\"dataset/val\", transform=val_transform)\n",
    "# REPLACE ./dataset/dataset WITH THE FOLDER WHERE YOU DOWNLOADED AND UNZIPPED THE DATASET\n",
    "# OR USE torchvision.datasets.ImageFolder INSTEAD OF MyDataset\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=16,\n",
    "    shuffle=True, \n",
    "    num_workers=0)\n",
    "    \n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=16,\n",
    "    shuffle=False, \n",
    "    num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "cell_id": 6,
    "id": "mrg4Yj0VlRVP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tests passed\n"
     ]
    }
   ],
   "source": [
    "# Just very simple sanity checks\n",
    "assert isinstance(train_dataset[0], tuple)\n",
    "assert len(train_dataset[0]) == 2\n",
    "assert isinstance(train_dataset[1][1], int)\n",
    "print(\"tests passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/5b/j4k5qtvx2j7b9nzkp2clls4c0000gp/T/ipykernel_70055/721719146.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;31m# create grid of images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mimg_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "        \n",
    "images, labels = next(iter(train_dataloader))\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "# create grid of images\n",
    "img_grid = torchvision.utils.make_grid(images)\n",
    "# show images\n",
    "matplotlib_imshow(img_grid, one_channel=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "output with shape [1, 64, 64] doesn't match the broadcast shape [3, 64, 64]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/5b/j4k5qtvx2j7b9nzkp2clls4c0000gp/T/ipykernel_70055/1170397846.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m    \u001b[0;31m# imgs, labels = imgs.to(device), labels.to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/5b/j4k5qtvx2j7b9nzkp2clls4c0000gp/T/ipykernel_70055/1706883771.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNormalized\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \"\"\"\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m     \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: output with shape [1, 64, 64] doesn't match the broadcast shape [3, 64, 64]"
     ]
    }
   ],
   "source": [
    "for imgs, labels in tqdm(train_dataloader,leave=False):\n",
    "    print(labels)\n",
    "   # imgs, labels = imgs.to(device), labels.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8RlSlmyjlRVP"
   },
   "source": [
    "### Вспомогательные функции, реализация модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "cell_id": 7,
    "id": "yYG2-Cq8lRVQ"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_dataloader, criterion, optimizer, device=\"cuda:0\", epoch=0):\n",
    "    model.train()\n",
    "    for imgs, labels in tqdm(train_dataloader, desc=f\"Training, epoch {epoch}\", leave=False):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "        y_pred = model(imgs)\n",
    "        loss = criterion(y_pred, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "\n",
    "        # log loss for the current epoch and the whole training history\n",
    "        train_epoch_loss = torch.cat((train_epoch_loss, loss.unsqueeze(0) / labels.size(0)))\n",
    "        train_loss_log.append(loss.data / labels.size(0))\n",
    "\n",
    "        # log accuracy for the current epoch and the whole training history\n",
    "        pred_classes = torch.argmax(y_pred, dim=-1)\n",
    "        train_epoch_true_hits = torch.cat((\n",
    "            train_epoch_true_hits, \n",
    "            (pred_classes == labels).sum().unsqueeze(0)\n",
    "        ))\n",
    "        train_acc_log.append((pred_classes == labels).cpu().sum() / labels.shape[0])\n",
    "\n",
    "\n",
    "    # валидация\n",
    "    val_epoch_loss, val_epoch_true_hits = torch.empty(0), torch.empty(0)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in tqdm(val_dataloader, desc=f\"Validating, epoch {epoch}\", leave=False):\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "            y_pred = model(imgs)\n",
    "            loss = criterion(y_pred, labels)\n",
    "            val_epoch_loss = torch.cat((val_epoch_loss, loss.unsqueeze(0) / labels.size(0)))\n",
    "\n",
    "            pred_classes = torch.argmax(y_pred, dim=-1)\n",
    "            val_epoch_true_hits = torch.cat((\n",
    "                val_epoch_true_hits,\n",
    "                (pred_classes == labels).sum().unsqueeze(0)\n",
    "            ))\n",
    "\n",
    "\n",
    "    val_loss_log.append(val_epoch_loss.mean())\n",
    "    val_acc_log.append(val_epoch_true_hits.sum() / val_epoch_true_hits.size(0) / val_dataloader.batch_size)\n",
    "    clear_output()\n",
    "    plot_history(train_loss_log, val_loss_log, \"loss\")\n",
    "    plot_history(train_acc_log, val_acc_log, \"accuracy\")\n",
    "\n",
    "    print(\"Train loss:\", train_epoch_loss.mean().item())\n",
    "    print(\n",
    "        \"Train acc:\", \n",
    "        (train_epoch_true_hits.sum() / train_epoch_true_hits.size(0) / train_dataloader.batch_size).item()\n",
    "    )\n",
    "    print(\"Val loss:\", val_epoch_loss.mean().item())\n",
    "    print(\n",
    "        \"Val acc:\", \n",
    "        (val_epoch_true_hits.sum() / val_epoch_true_hits.size(0) / val_dataloader.batch_size).item()\n",
    "    )\n",
    "#     pass\n",
    "\n",
    "\n",
    "def predict(model, val_dataloder, criterion, device=\"cuda:0\"):\n",
    "    model.eval()\n",
    "    # YOUR CODE\n",
    "    # PREDICT FOR EVERY ELEMENT OF THE VAL DATALOADER AND RETURN CORRESPONDING LISTS\n",
    "    return losses, predicted_classes, true_classes\n",
    "\n",
    "\n",
    "def train(model, train_dataloader, val_dataloader, criterion, optimizer, device=\"cuda:0\", n_epochs=10, scheduler=None):\n",
    "    model.to(device)\n",
    "    for epoch in range(n_epochs):\n",
    "        train_one_epoch(model, train_dataloader, criterion, optimizer, device=\"cuda:0\", epoch=epoch)\n",
    "        # Train, evaluate, print accuracy, make a step of scheduler or whatever you want...\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MxR3gfcilRVW"
   },
   "source": [
    "### Обучение модели, запуски экспериментов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "cell_id": 8,
    "id": "JXFJ6oS8lRVX",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "# model = nn.Sequential(\n",
    "#     nn.Conv2d(3, 16, 3, padding='same'),\n",
    "#     nn.BatchNorm2d(16),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Conv2d(16, 16, 3, padding='same'),\n",
    "#     nn.BatchNorm2d(16),\n",
    "#     nn.ReLU(),\n",
    "#     nn.MaxPool2d(2, 2),\n",
    "#     nn.Dropout(0.2),\n",
    "\n",
    "#     nn.Conv2d(16, 32, 3, padding='same'),\n",
    "#     nn.BatchNorm2d(32),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Conv2d(32, 32, 3, padding=1),\n",
    "#     nn.BatchNorm2d(32),\n",
    "#     nn.ReLU(),\n",
    "#     nn.MaxPool2d(2, 2),\n",
    "#     nn.Dropout(0.2),\n",
    "\n",
    "#     nn.Conv2d(32, 64, 3, padding=1),\n",
    "#     nn.BatchNorm2d(64),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Conv2d(64, 64, 3, padding=1),\n",
    "#     nn.BatchNorm2d(64),\n",
    "#     nn.ReLU(),\n",
    "#     nn.MaxPool2d(2, 2),\n",
    "#     nn.Dropout(0.2),\n",
    "\n",
    "#     nn.Flatten(), \n",
    "#     nn.Linear(64 * 4 * 4, 128),\n",
    "#     nn.BatchNorm1d(128),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Dropout(0.5),\n",
    "#     nn.Linear(128, 200),\n",
    "# )\n",
    "\n",
    "# model = nn.Sequential(\n",
    "#     nn.Conv2d(3, 16, 3, padding='same'),\n",
    "#     nn.BatchNorm2d(16),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Conv2d(16, 16, 3, padding='same'),\n",
    "#     nn.BatchNorm2d(16),\n",
    "#     nn.ReLU(),\n",
    "#     nn.MaxPool2d(2, 2),\n",
    "#     nn.Dropout(0.2),\n",
    "    \n",
    "#     nn.Flatten(), \n",
    "#     nn.Linear(64 * 4 * 4, 128),\n",
    "#     nn.BatchNorm1d(128),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Dropout(0.5),\n",
    "#     nn.Linear(128, 200),\n",
    "# )\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Flatten(),             # превращаем картинку 28х28 в вектор размером 784\n",
    "    nn.Linear(64 * 64, 256),  # линейный слой, преобразующий вектор размера 784 в вектор размера 128\n",
    "    nn.ReLU(),                # нелинейность\n",
    "    nn.Linear(256, 2000),       # линейный слой, преобразующий вектор размера 128 в вектор размера 10\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "# optimizer = torch.optim.Adam(model.fc.parameters(), 1e-4)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "n_epochs = 10\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (16x12288 and 4096x256)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/5b/j4k5qtvx2j7b9nzkp2clls4c0000gp/T/ipykernel_70055/2004583627.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/5b/j4k5qtvx2j7b9nzkp2clls4c0000gp/T/ipykernel_70055/2360263911.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, train_dataloader, criterion, optimizer, device, epoch)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1845\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (16x12288 and 4096x256)"
     ]
    }
   ],
   "source": [
    "train_one_epoch(model, train_dataloader, criterion, optimizer, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": 9,
    "id": "CesoOl6BlRVY"
   },
   "source": [
    "Простой тест на проверку правильности написанного кода"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 10,
    "id": "B_LB2jn6lRVY"
   },
   "outputs": [],
   "source": [
    "all_losses, predicted_labels, true_labels = predict(model, val_dataloader, criterion, device)\n",
    "assert len(predicted_labels) == len(val_dataset)\n",
    "accuracy = accuracy_score(predicted_labels, true_labels)\n",
    "print(\"tests passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": 11,
    "id": "tS-LLiXUlRVY"
   },
   "source": [
    "Запустить обучение можно в ячейке ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 12,
    "id": "ECIzZ_RYlRVZ"
   },
   "outputs": [],
   "source": [
    "train(model, train_dataloader, val_dataloader, criterion, optimizer, device, n_epochs, scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ImVW8_EXlRVZ"
   },
   "source": [
    "### Проверка полученной accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": 13,
    "id": "FmR-elhJlRVZ"
   },
   "source": [
    "После всех экспериментов которые вы проделали, выберите лучшую из своих моделей, реализуйте и запустите функцию `evaluate`. Эта функция должна брать на вход модель и даталоадер с валидационными данными и возврашать accuracy, посчитанную на этом датасете."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": 14,
    "id": "3TGH0EFalRVb"
   },
   "outputs": [],
   "source": [
    "all_losses, predicted_labels, true_labels = predict(model, val_dataloader, criterion, device)\n",
    "assert len(predicted_labels) == len(val_dataset)\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "print(\"Оценка за это задание составит {} баллов\".format(min(5, 5 * accuracy / 0.44)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": 15,
    "id": "pT8vfPSolRVb"
   },
   "source": [
    "### Отчёт об экспериментах \n",
    "\n",
    "текст писать тут"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "hw01_part2_tony_upd.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "max_cell_id": 35
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
